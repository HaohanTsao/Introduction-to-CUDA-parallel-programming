# CUDA與GPU平行計算筆記

## 1. CPU與GPU架構差異
* **基本差異**：
   * GPU將更多晶體管用於運算單元(ALU)
   * CPU將更多晶體管用於控制單元和緩存
   * 這使GPU特別適合執行相同操作於大量數據的任務
* **關鍵理解**：
   * "GPU專為計算密集型、高度平行的計算而設計 — 正是圖形渲染所需的特性"
* **視覺化對比**：
   * CPU：少量強大的處理核心 + 大量控制邏輯和緩存
   * GPU：大量簡單的處理核心 + 少量控制邏輯

## 2. 記憶體架構差異
* **主機板 vs 顯示卡**：
   * 主機板：DDR4記憶體，較窄的數據通道
   * 顯示卡：GDDR5記憶體，更寬的記憶體頻寬
* **頻寬設計**：
   * GPU的記憶體系統專為高頻寬平行存取設計
   * 能夠同時處理更多數據，這對平行計算至關重要

## 3. GPU加速計算的基本概念
* **協處理器模型**：
   * CPU控制程式執行流程並準備數據
   * 將大規模平行計算任務發送到GPU
   * GPU處理完後結果返回給CPU
* **效率公式**：
   * 計算量 >> 數據傳輸量 = 高效GPU加速
* **數據傳輸注意事項**：
   * CPU→GPU數據複製是高開銷操作
   * 需要盡量減少CPU和GPU之間的數據傳輸
   * 儘可能在GPU上完成整個計算流程

## 4. GTX 1060規格分析
* **基本規格**：
   * CUDA核心：1280個
   * 多處理器：10個（每個128核心）
   * 全域記憶體：6GB
   * 計算能力：6.1
* **執行參數**：
   * 每區塊最大線程數：1024
   * 共享記憶體每區塊：49152 bytes
   * 線程束大小：32
   * 每個多處理器最大線程數：2048
* **對編程的影響**：
   * 這些參數決定了如何最佳配置區塊和線程
   * 影響共享記憶體的使用策略

## 5. P2P通信技術
* **P2P通信定義**：
   * 允許一個GPU直接存取另一個GPU的記憶體
   * 不需要通過CPU和主記憶體作為中介
* **優勢**：
   * 減少數據傳輸延遲
   * 提高頻寬利用率
   * 減輕CPU負擔
* **實際應用**：
   * 使用`cudaMemcpyPeer()`函數直接在GPU之間複製數據
   * 使用`cudaDeviceEnablePeerAccess()`函數啟用P2P存取

## 6. 多GPU連接拓撲
* **立方體拓撲**：
   * 8個GPU位於立方體的8個頂點
   * 通過邊和特定的面對角線連接
   * 優化任意兩個GPU間的通信路徑
* **NVLink技術**：
   * NVLink 2.0提供約300 GB/s的總數據傳輸率
   * 遠超PCIe連接提供的頻寬
   * 對需要大量GPU間數據移動的應用至關重要

## 7. GPU代際演進（A100 vs V100）
* **性能提升**：
   * 標準配置：A100比V100快73%
   * 加L2優化：A100比V100快148%
* **技術進步**：

| 特性 | V100 | A100 | 提升 |
|------|------|------|------|
| 製程 | 12nm | 7nm | 更小更高效 |
| 晶體管數量 | 210億 | 547億 | +160% |
| CUDA核心 | 5120 | 6912 | +35% |
| 記憶體頻寬 | 900 GB/s | 1.6 TB/s | +78% |
| L2快取 | 6MB | 40MB | +567% |

* **新增技術**：
   * 第三代Tensor Cores
   * 稀疏矩陣加速
   * TF32和BF16格式支援
   * Multi-Instance GPU (MIG)技術

## 8. 格點QCD計算基準測試
* **測試環境**：
   * 格點大小：64³ × 64 × 16
   * 使用DWF (Domain Wall Fermion)算法
* **測試結果**：
   * 8個A100在混合精度CG測試達到26474 GFLOPS
   * 顯著優於8個V100的10939 GFLOPS
* **混合精度優勢**：
   * 結合單精度計算速度與雙精度精確度
   * 主體使用單精度加速，關鍵步驟用雙精度

## 9. CUDA編程模型
* **核心概念**：
   * "CUDA是一個可擴展的平行程式設計模型和軟體環境，設計用於開發能夠透明擴展其平行性以利用不斷增加的核心數量的代碼"
* **C語言擴展**：
   * CUDA C只是C語言的擴展
   * 添加少量關鍵字和函數，利用GPU平行計算能力
   * 學習曲線相對平緩
* **工作原理**：
   * 計算任務被組織為大量平行執行的線程
   * 線程分組為線程塊，再組織為網格
   * 這種層次結構對應GPU硬體架構

## 10. CUDA架構詳解
* **執行層次**：
   * Thread：最小執行單位，有自己的寄存器和局部變數
   * Block：多個線程組成，可通過共享記憶體通信
   * Grid：多個區塊的集合，構成完整的執行環境
* **記憶體層次**：
   * Shared Memory：區塊內線程共享，高速但容量有限
   * Constant Memory/Texture Cache：只讀，適合特定訪問模式
   * Global Memory：所有線程可訪問，容量大但速度慢
   * Host Memory：CPU的RAM，通過PCIe與GPU通信

## 11. GPU記憶體層次詳解
* **五種記憶體類型**：

| 類型 | 容量 | 存取權限 | 頻寬 |
|------|------|---------|------|
| Global | 大型 | 所有線程和主機讀寫 | 慢 |
| Constant | 小型 | 所有線程唯讀 | 快 |
| Texture | 小型 | 所有線程唯讀 | 快 |
| Shared | 很小 | 同區塊線程讀寫 | 很快 |
| Registers | 很小 | 單一線程讀寫 | 很快 |

* **關鍵注意事項**：
   * Shared Memory可能有bank conflicts問題
   * Texture Memory優化空間局部性訪問
   * **GPU計算受記憶體頻寬限制！**

## 12. 線程層次結構
* **線程識別**：
  * `threadIdx`：線程在區塊內的位置（x, y, z三個維度）
  * `blockIdx`：區塊在網格中的位置（x, y, z三個維度）
  * `blockDim`：區塊的尺寸（x, y, z三個維度的線程數）
* **全局位置計算**：
  * 一維: `globalIdx = blockIdx.x * blockDim.x + threadIdx.x`
  * 二維: `globalIdx_y = blockIdx.y * blockDim.y + threadIdx.y`
* **多維組織的優勢**：
  * 自然映射到多維數據結構（如矩陣、圖像）
  * 提高代碼可讀性和維護性

## 13. 執行模型特性
* **透明擴展性**：
  * 硬體可以自由地將線程區塊調度到任何處理器
  * 同一程式可在不同能力的GPU上自動調整執行方式
* **區塊執行特性**：
  * 一個區塊在一個多處理器上執行，不會遷移
  * 一個多處理器可同時執行多個區塊
  * 區塊數量受多處理器資源限制
* **資源分配**：
  * Registers在多處理器上的所有線程間分配
  * Shared Memory在多處理器上的所有區塊間分配

## 14. 區塊內線程協作
* **Shared Memory**：
  * 同一區塊內的線程可通過Shared Memory交換數據
  * 比Global Memory快數十倍，但容量有限
* **同步機制**：
  * `__syncthreads()`：區塊內的同步屏障
  * 所有線程必須到達這個點才能繼續執行
* **協作限制**：
  * 不同區塊的線程無法直接協作
  * 需要通過Global Memory和多次內核調用實現跨區塊協作

## 15. 向上取整技巧
* **計算區塊數量**：
  * 使用公式: `(N + BLOCK_SIZE - 1) / BLOCK_SIZE`
  * 確保有足夠的區塊覆蓋所有數據元素
* **工作原理**：
  * 當N能被BLOCK_SIZE整除時，結果不變
  * 當N不能被BLOCK_SIZE整除時，結果會向上取整
* **應用場景**：
  * 計算網格尺寸: `dim3 dimGrid((N+BLOCK_SIZE-1)/BLOCK_SIZE, (N+BLOCK_SIZE-1)/BLOCK_SIZE);`

## 16. 從二維到一維的索引轉換
* **行主序排列**：
  * 使用公式: `index = i * N + j`
  * 其中i是行索引，j是列索引，N是每行的元素數
* **具體例子**：
  * 4×4矩陣中，元素(1,2)的一維索引是 1*4+2=6
  * 這對應矩陣第二行第三列的元素
* **在CUDA中的應用**：
  * 全域索引計算後，通過此公式訪問線性記憶體中的二維數據

## 17. 矩陣加法範例解析
* **核函數定義**：
  ```cuda
  __global__ void matAdd(float *A, float *B, float *C, int N) {
      int i = blockIdx.y * blockDim.y + threadIdx.y;
      int j = blockIdx.x * blockDim.x + threadIdx.x;
      if (i < N && j < N) {
          int index = i * N + j;
          C[index] = A[index] + B[index];
      }
  }
  ```
* **執行配置**：
  ```cuda
  dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);
  dim3 dimGrid((N+BLOCK_SIZE-1)/BLOCK_SIZE, (N+BLOCK_SIZE-1)/BLOCK_SIZE);
  matAdd<<<dimGrid, dimBlock>>>(d_A, d_B, d_C, N);
  ```
* **關鍵步驟**：
  1. 計算全域索引(i,j)
  2. 檢查邊界條件
  3. 將二維索引轉換為一維
  4. 執行元素加法操作

## 18. CUDA記憶體管理
* **分配與釋放**：
  * `cudaMalloc(void** devPtr, size_t size)`：分配設備記憶體
  * `cudaFree(void* devPtr)`：釋放設備記憶體
* **數據傳輸**：
  * `cudaMemcpy(void* dst, const void* src, size_t count, cudaMemcpyKind kind)`
  * 傳輸方向通過`kind`參數指定：
    * `cudaMemcpyHostToDevice`：CPU到GPU
    * `cudaMemcpyDeviceToHost`：GPU到CPU
    * `cudaMemcpyDeviceToDevice`：GPU內部或GPU間（P2P）
* **最佳實踐**：
  * 盡量減少主機和設備間的數據傳輸
  * 考慮使用pinned主機記憶體加速傳輸
  * 使用異步傳輸和streams重疊計算與傳輸

## 19. 優化策略
* **計算密度優化**：
  * 增加每個線程的工作量，減少區塊數量
  * 在內核中使用迴圈處理多個數據元素
* **記憶體訪問優化**：
  * 合併記憶體訪問（coalesced access）
  * 使用Shared Memory作為Global Memory的快取
  * 避免bank conflicts
* **occupancy管理**：
  * 平衡每個線程的資源使用
  * 使用`cudaOccupancyMaxPotentialBlockSize`函數找出最佳區塊大小

## 20. 性能評估與調試
* **性能計時**：
  * 使用`cudaEvent_t`測量GPU執行時間
  * 對比不同實現方式的效能
* **調試工具**：
  * Nsight Compute：性能分析
  * Nsight Systems：系統級跟踪
  * `printf`在CUDA核函數中的使用
* **常見性能瓶頸**：
  * 記憶體頻寬限制
  * Register pressure
  * Warp divergence

## 21. 本週重點整理

1. **CUDA基礎架構**：理解GPU與CPU的核心差異，GPU專注於大規模平行計算，擁有大量簡單處理核心和高頻寬記憶體系統。

2. **執行模型**：CUDA程式以Thread、Block和Grid的層次結構組織，每個層次有特定的功能和限制：
   * Thread：最基本的執行單位，有自己的ID和局部資源
   * Block：線程的集合，可通過Shared Memory協作，在單一多處理器上執行
   * Grid：Block的集合，覆蓋整個數據集，實現透明擴展

3. **索引計算**：掌握從線程和區塊ID到全域位置的計算，以及從二維到一維索引的轉換，是編寫正確CUDA程式的基礎。

4. **記憶體管理**：理解CUDA中的各種記憶體類型及其特性，合理使用Global、Shared、Constant和Register記憶體，對優化程式性能至關重要。

5. **優化思路**：CUDA程式最常見的瓶頸是記憶體頻寬，而非計算能力。優化應著重於提高記憶體訪問效率、增加計算密度和管理好資源佔用。

6. **硬體發展趨勢**：隨著GPU架構不斷演進（如從V100到A100），新技術如NVLink、P2P通信、Tensor Cores等提供了更多優化可能性，但基本的CUDA編程模型和優化原則仍然適用。

7. **實作思考**：編寫CUDA程式時，需要找到適合問題特性的平行分解方式，選擇合適的Block大小和組織方式，並注意數據局部性和記憶體訪問模式。

記住：CUDA編程的藝術在於理解硬體能力和限制，然後據此組織軟體結構，實現計算與記憶體訪問的最佳平衡。

## 22. 基礎架構知識補充

1. **記憶體層次結構**
   - **Global Memory**：位於晶片外，容量大但訪問較慢，所有線程可訪問
   - **Shared Memory**：位於晶片上，高速但容量有限，僅在同一Block內的線程間共享
   - **Registers**：速度最快，每個線程獨有

2. **CUDA執行模型**
   - **Grid**：整個計算任務，由多個Block組成
   - **Block**：計算任務的一部分，可以是一維、二維或三維排列
   - **Thread**：最基本的執行單位，每個Thread執行相同程式碼但處理不同數據

## 23. 優化原則補充

1. **記憶體傳輸優化**
   - 減少主機(CPU)到設備(GPU)間的數據傳輸
   - 利用Shared/Constant/Texture記憶體作為高速緩存
   - 設計程式以實現coalesced memory access

2. **處理器佔用率最大化**
   - 選擇最佳Block大小（通常是32的倍數）
   - 平衡每個Block的資源使用

3. **計算密度最大化**
   - 增加每次記憶體訪問的計算量
   - 在某些情況下，重新計算可能比重新加載數據更高效

## 24. 實用技巧補充

1. **性能分析與調整**
   - 使用專業工具（如Nsight）測量程式性能
   - 通過實驗測試尋找最佳參數設置

2. **資源平衡**
   - 在CUDA編程中，需要平衡考慮記憶體使用、計算效率和資源佔用
   - 針對特定GPU架構，某些優化策略會更有效